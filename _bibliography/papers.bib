---
---

@article{higgins2021actionable,
      title={Actionable Conversational Quality Indicators for Improving Task-Oriented Dialog Systems}, 
      author={Michael Higgins and Dominic Widdows and Chris Brew and Gwen Christian and Andrew Maurer and Matthew Dunn and Sujit Mathi and Akshay Hazare and George Bonev and Beth Ann Hockey and Kristen Howell and Joe Bradley},
      year={2021},
      bibtex_show={true},
      eprint={2109.11064},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{dunn2017searchqa,
      title={SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine}, 
      author={Matthew Dunn and Levent Sagun and Mike Higgins and V. Ugur Guney and Volkan Cirik and Kyunghyun Cho},
      year={2017},
      eprint={1704.05179},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{10.1145/3086512.3086537,
  author = {Dunn, Matt and Sagun, Levent and \c{S}irin, Hale and Chen, Daniel},
  title = {Early Predictability of Asylum Court Decisions},
  year = {2017},
  isbn = {9781450348911},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3086512.3086537},
  doi = {10.1145/3086512.3086537},
  booktitle = {Proceedings of the 16th Edition of the International Conference on Articial Intelligence and Law},
  pages = {233â€“236},
  numpages = {4},
  keywords = {judicial analytics, judicial decision-making, snap judgments},
  location = {London, United Kingdom},
  series = {ICAIL '17}
}

@inproceedings{howell-etal-2022-domain,
    title = "Domain-specific knowledge distillation yields smaller and better models for conversational commerce",
    author = "Howell, Kristen  and
      Wang, Jian  and
      Hazare, Akshay  and
      Bradley, Joseph  and
      Brew, Chris  and
      Chen, Xi  and
      Dunn, Matthew  and
      Hockey, Beth  and
      Maurer, Andrew  and
      Widdows, Dominic",
    booktitle = "Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.ecnlp-1.18",
    doi = "10.18653/v1/2022.ecnlp-1.18",
    pages = "151--160",
    abstract = "We demonstrate that knowledge distillation can be used not only to reduce model size, but to simultaneously adapt a contextual language model to a specific domain. We use Multilingual BERT (mBERT; Devlin et al., 2019) as a starting point and follow the knowledge distillation approach of (Sahn et al., 2019) to train a smaller multilingual BERT model that is adapted to the domain at hand. We show that for in-domain tasks, the domain-specific model shows on average 2.3{\%} improvement in F1 score, relative to a model distilled on domain-general data. Whereas much previous work with BERT has fine-tuned the encoder weights during task training, we show that the model improvements from distillation on in-domain data persist even when the encoder weights are frozen during task training, allowing a single encoder to support classifiers for multiple tasks and languages.",
}